{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import logging\n",
    "import re\n",
    "import csv\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textbook_path = '../tw_clean.txt'\n",
    "#df = pd.read_csv(textbook_path, sep='\\n', names=['documents'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_corpus(fname):\n",
    "    with open(fname) as f:\n",
    "        lines = iter(f)\n",
    "        #lines.next()\n",
    "        for line in lines:\n",
    "            yield line\n",
    "\n",
    "class CorpusFriendly(object):\n",
    "    def __init__(self,fname, tag=True, topK=None):\n",
    "        self.topK = topK\n",
    "        self.fname = fname\n",
    "        self.tag = tag\n",
    "    def __iter__(self):\n",
    "        k = 1\n",
    "        corpus_memory_friendly = get_corpus(self.fname)\n",
    "        for context in corpus_memory_friendly:\n",
    "            #data = gensim.utils.to_unicode(data).split(',')\n",
    "            seg_context = jieba.cut(context, cut_all=True)\n",
    "            words = \" \".join(seg_context).split()\n",
    "            #print \", \".join(words)\n",
    "            label = [str(k)]\n",
    "            if self.tag:\n",
    "                yield gensim.models.doc2vec.TaggedDocument(words, label)\n",
    "            else:\n",
    "                yield words\n",
    "            if self.topK:\n",
    "                if k >= self.topK:\n",
    "                    break\n",
    "                k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corpus_train = CorpusFriendly(fname='hk_clean.txt',topK=3)\n",
    "# for c in corpus_train:\n",
    "#     print \", \".join(c.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for c in get_corpus(textbook_path):\n",
    "#     print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus for Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "for c in get_corpus(textbook_path):\n",
    "    text.extend(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_sentences = []\n",
    "for t in \"\".join(text).split('\\n'):\n",
    "    text_sentences.extend(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_word2vec = []\n",
    "for t in text_sentences:\n",
    "    corpus_word2vec.extend(t.split('。'))\n",
    "corpus_word2vec = filter(None, corpus_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2570\n"
     ]
    }
   ],
   "source": [
    "print len(corpus_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../tw_clean_sentences.txt\", 'wb') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter='\\n')\n",
    "    for t in corpus_word2vec:\n",
    "        writer.writerow([t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 2000\n",
    "min_word_count = 1\n",
    "num_workers = 4\n",
    "context = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 14:13:31,599 : INFO : collecting all words and their counts\n",
      "2016-12-29 14:13:31,601 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-12-29 14:13:31,856 : INFO : collected 12223 word types from a corpus of 70494 raw words and 2570 sentences\n",
      "2016-12-29 14:13:31,884 : INFO : min_count=1 retains 12223 unique words (drops 0)\n",
      "2016-12-29 14:13:31,884 : INFO : min_count leaves 70494 word corpus (100% of original 70494)\n",
      "2016-12-29 14:13:31,917 : INFO : deleting the raw counts dictionary of 12223 items\n",
      "2016-12-29 14:13:31,918 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2016-12-29 14:13:31,918 : INFO : downsampling leaves estimated 65230 word corpus (92.5% of prior 70494)\n",
      "2016-12-29 14:13:31,919 : INFO : estimated required memory for 12223 words and 2000 dimensions: 201679500 bytes\n",
      "2016-12-29 14:13:31,946 : INFO : resetting layer weights\n",
      "2016-12-29 14:13:32,435 : INFO : training model with 4 workers on 12223 vocabulary and 2000 features, using sg=0 hs=0 sample=0.001 negative=5\n",
      "2016-12-29 14:13:32,435 : INFO : expecting 2570 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 14:13:33,438 : INFO : PROGRESS: at 11.95% examples, 156709 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:34,448 : INFO : PROGRESS: at 25.45% examples, 165508 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:35,483 : INFO : PROGRESS: at 39.70% examples, 170029 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:36,505 : INFO : PROGRESS: at 53.04% examples, 170279 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:37,537 : INFO : PROGRESS: at 65.79% examples, 168503 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:38,541 : INFO : PROGRESS: at 77.86% examples, 166457 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:39,584 : INFO : PROGRESS: at 91.29% examples, 166720 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:40,135 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 14:13:40,137 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 14:13:40,191 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 14:13:40,244 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 14:13:40,245 : INFO : training on 1409880 raw words (1304523 effective words) took 7.8s, 167094 effective words/s\n"
     ]
    }
   ],
   "source": [
    "corpus_train = CorpusFriendly(fname=\"../tw_clean_sentences.txt\", tag=False)\n",
    "model_word2vec = gensim.models.word2vec.Word2Vec(corpus_train, \n",
    "                                        size=num_features, \n",
    "                                        min_count=min_word_count, \n",
    "                                        iter=20, \n",
    "                                        window=context, \n",
    "                                        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 14:13:44,829 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "派对 0.997701048851\n",
      "般 0.997641205788\n",
      "时局 0.997374773026\n",
      "少年 0.997246026993\n",
      "这 0.996866822243\n",
      "卫兵 0.996851146221\n",
      "群众路线 0.996850311756\n",
      "通常 0.996457397938\n",
      "围观 0.996254146099\n",
      "红卫兵 0.99623644352\n"
     ]
    }
   ],
   "source": [
    "result_word2vec = model_word2vec.most_similar(positive=[u'毛泽东'], topn=10)\n",
    "for e in result_word2vec:\n",
    "    print e[0], e[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 14:13:51,869 : INFO : collecting all words and their counts\n",
      "2016-12-29 14:13:51,953 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2016-12-29 14:13:52,111 : INFO : collected 12223 word types and 1 unique tags from a corpus of 267 examples and 70494 words\n",
      "2016-12-29 14:13:52,185 : INFO : min_count=1 retains 12223 unique words (drops 0)\n",
      "2016-12-29 14:13:52,186 : INFO : min_count leaves 70494 word corpus (100% of original 70494)\n",
      "2016-12-29 14:13:52,215 : INFO : deleting the raw counts dictionary of 12223 items\n",
      "2016-12-29 14:13:52,216 : INFO : sample=0 downsamples 0 most-common words\n",
      "2016-12-29 14:13:52,216 : INFO : downsampling leaves estimated 70494 word corpus (100.0% of prior 70494)\n",
      "2016-12-29 14:13:52,217 : INFO : estimated required memory for 12223 words and 2000 dimensions: 204132300 bytes\n",
      "2016-12-29 14:13:52,232 : INFO : constructing a huffman tree from 12223 words\n",
      "2016-12-29 14:13:52,564 : INFO : built huffman tree with maximum node depth 16\n",
      "2016-12-29 14:13:52,569 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "corpus_train = CorpusFriendly(fname=textbook_path)\n",
    "model_doc2vec = gensim.models.doc2vec.Doc2Vec(size=2000, min_count=1, iter=30, window=10, workers=6)\n",
    "model_doc2vec.build_vocab(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 14:13:53,748 : INFO : training model with 6 workers on 12223 vocabulary and 2000 features, using sg=0 hs=1 sample=0 negative=0\n",
      "2016-12-29 14:13:53,748 : INFO : expecting 267 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 14:13:54,934 : INFO : PROGRESS: at 4.11% examples, 79217 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:55,950 : INFO : PROGRESS: at 10.80% examples, 99632 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:56,965 : INFO : PROGRESS: at 20.04% examples, 106243 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:13:58,007 : INFO : PROGRESS: at 24.13% examples, 100966 words/s, in_qsize 1, out_qsize 0\n",
      "2016-12-29 14:13:59,176 : INFO : PROGRESS: at 31.95% examples, 100017 words/s, in_qsize 3, out_qsize 0\n",
      "2016-12-29 14:14:00,222 : INFO : PROGRESS: at 38.63% examples, 101582 words/s, in_qsize 2, out_qsize 0\n",
      "2016-12-29 14:14:01,287 : INFO : PROGRESS: at 45.29% examples, 101260 words/s, in_qsize 3, out_qsize 0\n",
      "2016-12-29 14:14:02,352 : INFO : PROGRESS: at 53.38% examples, 102093 words/s, in_qsize 2, out_qsize 0\n",
      "2016-12-29 14:14:03,386 : INFO : PROGRESS: at 60.79% examples, 104152 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:14:04,394 : INFO : PROGRESS: at 67.47% examples, 104696 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:14:05,395 : INFO : PROGRESS: at 74.13% examples, 104741 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:14:06,408 : INFO : PROGRESS: at 80.80% examples, 104678 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:14:07,456 : INFO : PROGRESS: at 87.47% examples, 104363 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 14:14:08,476 : INFO : PROGRESS: at 94.11% examples, 102921 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-29 14:14:08,562 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 14:14:08,616 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 14:14:08,619 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 14:14:08,706 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 14:14:08,781 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 14:14:08,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 14:14:08,823 : INFO : training on 2114820 raw words (1579800 effective words) took 15.1s, 104818 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1579800"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_doc2vec.train(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_doc2vec.save('models/gensim_doc2vec_hk_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 14:10:25,190 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大会 0.456900596619\n",
      "中央 0.446250140667\n",
      "召开 0.441353291273\n",
      "邓小平 0.43362864852\n",
      "七届 0.428785741329\n",
      "文章 0.401267051697\n",
      "救亡图存 0.397203177214\n",
      "修正 0.390192836523\n",
      "会议 0.385841727257\n",
      "领导 0.374116033316\n",
      "三个代表 0.373360186815\n",
      "报告 0.370967835188\n",
      "错误 0.369134247303\n",
      "李大钊 0.36832010746\n",
      "决定 0.365838527679\n",
      "过渡时期 0.365819334984\n",
      "两万多 0.360181629658\n",
      "举行 0.354957491159\n",
      "代表人 0.351537585258\n",
      "思想 0.350723713636\n"
     ]
    }
   ],
   "source": [
    "result_doc2vec = model_doc2vec.most_similar(positive=[u'毛泽东'], topn=20)\n",
    "for e in result_doc2vec:\n",
    "    print e[0], e[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top N adjective from similar words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba.posseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MostSimilarGenerator\n",
    "MostSimilarGenerator is a generator to efficiently find all the adj and adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MostSimilarGenerator(object):\n",
    "    def __init__(self, model, word=u'毛泽东'):\n",
    "        self.word= word\n",
    "        self.model= model\n",
    "    def __iter__(self):\n",
    "        for sim_word in self.model.most_similar(self.word, topn=len(self.model.vocab)):\n",
    "            yield sim_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_adj()\n",
    "is_adj() will return True if input is an adjective, an adverb or an idiom, otherwise False\n",
    "\n",
    "jieba.posseg actually has ability to tag which part of speech is. I choose to tag adjective and adverb. Instead of using DictionaryServices, I find it makes more sense. However, we can always change the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_adj(word):\n",
    "    word = jieba.posseg.cut(word)\n",
    "    for w, f in word:\n",
    "        if 'a' in f or 'd' in f or 'i' in f:\n",
    "            return True\n",
    "    #print w, f\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_topn_adj()\n",
    "find_topn_adj() will return a list of most similar words and corresponding scores of the word of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_topn_adj(n, model, word=u'毛泽东',):\n",
    "    k = 0\n",
    "    topn_adj = []\n",
    "    topn_score = []\n",
    "    for sim_word in MostSimilarGenerator(model, word=word):\n",
    "        if is_adj(sim_word[0]):\n",
    "            topn_adj.append(sim_word[0])\n",
    "            topn_score.append(sim_word[1])\n",
    "            k += 1\n",
    "        if k >= n:\n",
    "            return zip(topn_adj, topn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "独秀 0.878687381744\n",
      "令人鼓舞 0.859706223011\n",
      "持久 0.83676224947\n",
      "拨乱反正 0.793787062168\n",
      "偏听偏信 0.792282879353\n",
      "错误思想 0.781506538391\n",
      "反正 0.779723525047\n",
      "解放思想 0.775874376297\n",
      "最低 0.77518415451\n",
      "平明 0.744004607201\n",
      "重新 0.73650676012\n",
      "大民 0.729310035706\n",
      "视死如归 0.719754815102\n",
      "老 0.711652576923\n",
      "幼稚 0.707704901695\n",
      "实际上 0.702308535576\n",
      "定将 0.700187206268\n",
      "极 0.696991801262\n",
      "平实 0.688151359558\n",
      "深 0.678931713104\n"
     ]
    }
   ],
   "source": [
    "for w, s in find_topn_adj(n=20, model= model_word2vec, word=u'毛泽东'):\n",
    "    print w, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "救亡图存 0.397203177214\n",
      "隆重 0.334757268429\n",
      "大打出手 0.312203615904\n",
      "正面 0.311421692371\n",
      "大手 0.305309802294\n",
      "一致 0.281828820705\n",
      "各口岸 0.270859718323\n",
      "明确 0.265751928091\n",
      "同意 0.246563345194\n",
      "远 0.233466386795\n",
      "正确 0.232836142182\n",
      "边 0.224828645587\n",
      "在此之前 0.217532545328\n",
      "之长 0.21311865747\n",
      "民富 0.212604701519\n",
      "反正 0.207456231117\n",
      "最低 0.207350760698\n",
      "严重错误 0.206992208958\n",
      "不再 0.20612719655\n",
      "星星之火 0.20379024744\n"
     ]
    }
   ],
   "source": [
    "for w, s in find_topn_adj(n=20, model= model_doc2vec, word=u'毛泽东'):\n",
    "    print w, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
