{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textbook_path = '../ma_clean.txt'\n",
    "#df = pd.read_csv(textbook_path, sep='\\n', names=['documents'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_corpus(fname):\n",
    "    with open(fname) as f:\n",
    "        lines = iter(f)\n",
    "        #lines.next()\n",
    "        for line in lines:\n",
    "            yield line\n",
    "\n",
    "class CorpusFriendly(object):\n",
    "    def __init__(self,fname, tag=True, topK=None):\n",
    "        self.topK = topK\n",
    "        self.fname = fname\n",
    "        self.tag = tag\n",
    "    def __iter__(self):\n",
    "        k = 1\n",
    "        corpus_memory_friendly = get_corpus(self.fname)\n",
    "        for context in corpus_memory_friendly:\n",
    "            #data = gensim.utils.to_unicode(data).split(',')\n",
    "            seg_context = jieba.cut(context, cut_all=True)\n",
    "            words = \" \".join(seg_context).split()\n",
    "            #print \", \".join(words)\n",
    "            label = [str(k)]\n",
    "            if self.tag:\n",
    "                yield gensim.models.doc2vec.TaggedDocument(words, label)\n",
    "            else:\n",
    "                yield words\n",
    "            if self.topK:\n",
    "                if k >= self.topK:\n",
    "                    break\n",
    "                k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corpus_train = CorpusFriendly(fname='hk_clean.txt',topK=3)\n",
    "# for c in corpus_train:\n",
    "#     print \", \".join(c.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for c in get_corpus(textbook_path):\n",
    "#     print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus for Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files exisit!!\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "clean_sentences = ['../tw_clean_sentences.txt', '../hk_clean_sentences.txt', '../ma_clean_sentences.txt']\n",
    "if reduce(lambda a, b: a*b,map(lambda x: os.path.exists(x), clean_sentences)):\n",
    "     print \"Files exisit!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "for c in get_corpus(textbook_path):\n",
    "    text.extend(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_sentences = []\n",
    "for t in \"\".join(text).split('\\n'):\n",
    "    text_sentences.extend(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_word2vec = []\n",
    "for t in text_sentences:\n",
    "    corpus_word2vec.extend(t.split('。'))\n",
    "corpus_word2vec = filter(None, corpus_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2570\n"
     ]
    }
   ],
   "source": [
    "print len(corpus_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../tw_clean_sentences.txt\", 'wb') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter='\\n')\n",
    "    for t in corpus_word2vec:\n",
    "        writer.writerow([t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 500\n",
    "min_word_count = 3\n",
    "num_workers = 4\n",
    "context = 6\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 15:41:25,814 : INFO : collecting all words and their counts\n",
      "2017-01-02 15:41:25,815 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-02 15:41:26,154 : INFO : collected 12907 word types from a corpus of 96272 raw words and 4010 sentences\n",
      "2017-01-02 15:41:26,170 : INFO : min_count=3 retains 4679 unique words (drops 8228)\n",
      "2017-01-02 15:41:26,171 : INFO : min_count leaves 85850 word corpus (89% of original 96272)\n",
      "2017-01-02 15:41:26,184 : INFO : deleting the raw counts dictionary of 12907 items\n",
      "2017-01-02 15:41:26,185 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-01-02 15:41:26,186 : INFO : downsampling leaves estimated 74380 word corpus (86.6% of prior 85850)\n",
      "2017-01-02 15:41:26,186 : INFO : estimated required memory for 4679 words and 500 dimensions: 21055500 bytes\n",
      "2017-01-02 15:41:26,197 : INFO : resetting layer weights\n",
      "2017-01-02 15:41:26,276 : INFO : training model with 4 workers on 4679 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5\n",
      "2017-01-02 15:41:26,276 : INFO : expecting 4010 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-02 15:41:27,290 : INFO : PROGRESS: at 8.65% examples, 190906 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:28,317 : INFO : PROGRESS: at 17.70% examples, 193004 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:29,329 : INFO : PROGRESS: at 26.62% examples, 194680 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:30,348 : INFO : PROGRESS: at 35.58% examples, 195270 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:31,387 : INFO : PROGRESS: at 44.68% examples, 194706 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:32,395 : INFO : PROGRESS: at 53.65% examples, 195427 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:33,419 : INFO : PROGRESS: at 62.55% examples, 195498 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:34,443 : INFO : PROGRESS: at 71.30% examples, 194557 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:35,457 : INFO : PROGRESS: at 80.26% examples, 194941 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:36,472 : INFO : PROGRESS: at 88.82% examples, 194449 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:37,475 : INFO : PROGRESS: at 97.55% examples, 194253 words/s, in_qsize 0, out_qsize 0\n",
      "2017-01-02 15:41:37,729 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-02 15:41:37,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-02 15:41:37,736 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-02 15:41:37,744 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-02 15:41:37,744 : INFO : training on 2888160 raw words (2230988 effective words) took 11.5s, 194576 effective words/s\n"
     ]
    }
   ],
   "source": [
    "corpus_train = CorpusFriendly(fname=\"../ma_clean_sentences.txt\", tag=False)\n",
    "model_word2vec = gensim.models.word2vec.Word2Vec(corpus_train, \n",
    "                                        size=num_features, \n",
    "                                        min_count=min_word_count, \n",
    "                                        iter=30, \n",
    "                                        window=context, \n",
    "                                        workers=4)\n",
    "                                        #sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-01 23:59:50,528 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "展开 0.770020604134\n",
      "毛 0.752099812031\n",
      "党 0.747189760208\n",
      "东路 0.743687391281\n",
      "党内 0.733431816101\n"
     ]
    }
   ],
   "source": [
    "result_word2vec = model_word2vec.most_similar(positive=[u'毛泽东'], topn=5)\n",
    "for e in result_word2vec:\n",
    "    print e[0], e[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corpus_train = CorpusFriendly(fname=textbook_path)\n",
    "# model_doc2vec = gensim.models.doc2vec.Doc2Vec(size=2000, min_count=1, iter=20, window=10, workers=6)\n",
    "# model_doc2vec.build_vocab(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_doc2vec.train(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_doc2vec.save('models/gensim_doc2vec_hk_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# result_doc2vec = model_doc2vec.most_similar(positive=[u'毛泽东'], topn=10)\n",
    "# for e in result_doc2vec:\n",
    "#     print e[0], e[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top N adjective from similar words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba.posseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MostSimilarGenerator\n",
    "MostSimilarGenerator is a generator to efficiently find all the adj and adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MostSimilarGenerator(object):\n",
    "    def __init__(self, model, word=u'毛泽东'):\n",
    "        self.word= word\n",
    "        self.model= model\n",
    "    def __iter__(self):\n",
    "        for sim_word in self.model.most_similar(self.word, topn=len(self.model.vocab)):\n",
    "            yield sim_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_adj()\n",
    "is_adj() will return True if input is an adjective, an adverb or an idiom, otherwise False\n",
    "\n",
    "jieba.posseg actually has ability to tag which part of speech is. I choose to tag adjective and adverb. Instead of using DictionaryServices, I find it makes more sense. However, we can always change the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_adj(word):\n",
    "    word = jieba.posseg.cut(word)\n",
    "    for w, f in word:\n",
    "        if 'a' in f or 'i' in f:\n",
    "            return True\n",
    "    #print w, f\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_topn_adj()\n",
    "find_topn_adj() will return a list of most similar words and corresponding scores of the word of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_topn_adj(n, model, word=u'毛泽东',):\n",
    "    k = 0\n",
    "    topn_adj = []\n",
    "    topn_score = []\n",
    "    for sim_word in MostSimilarGenerator(model, word=word):\n",
    "        if is_adj(sim_word[0]):\n",
    "            topn_adj.append(sim_word[0])\n",
    "            topn_score.append(sim_word[1])\n",
    "            k += 1\n",
    "        if k >= n:\n",
    "            #return zip(topn_adj, topn_score)\n",
    "            return {'word':topn_adj, 'score':topn_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "乐观 0.64469063282\n",
      "老 0.620627701283\n",
      "亲密 0.620467662811\n",
      "面红 0.602802634239\n",
      "最高 0.583639144897\n",
      "整肃 0.576191544533\n",
      "正确 0.547226250172\n",
      "公开 0.541342735291\n",
      "大党 0.539614021778\n",
      "言者无罪 0.530300855637\n",
      "牛鬼蛇神 0.506935596466\n",
      "独秀 0.499838531017\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# 蒋介石（蒋中正） 毛泽东\n",
    "adj_word2vec = []\n",
    "topn_adj = find_topn_adj(n=20, model= model_word2vec, word=u'毛泽东')\n",
    "for w, s in zip(topn_adj['word'], topn_adj['score']):\n",
    "    print w, s\n",
    "    adj_word2vec.append(w)\n",
    "    if s < 0.5:\n",
    "        break\n",
    "print len(adj_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adj_doc2vec = []\n",
    "# for w, s in find_topn_adj(n=200, model= model_doc2vec, word=u'毛泽东'):\n",
    "#     print w, s\n",
    "#     adj_doc2vec.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, math, collections, itertools\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4570\n"
     ]
    }
   ],
   "source": [
    "pos_files = ['../sentiment_dicts/正面情感词语（中文）.txt', \n",
    "             '../sentiment_dicts/正面评价词语（中文）.txt']\n",
    "             #'../sentiment_dicts/NTUSD_positive_simplified.txt']\n",
    "pos_words = []\n",
    "for p in pos_files:\n",
    "    with open(p) as posFile:\n",
    "        words = posFile.readlines()\n",
    "        words = list(map(lambda x:x.strip(),words))\n",
    "        #print len(posWords)\n",
    "        pos_words.extend(words)\n",
    "print len(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4374\n"
     ]
    }
   ],
   "source": [
    "neg_files = ['../sentiment_dicts/负面情感词语（中文）.txt', \n",
    "             '../sentiment_dicts/负面评价词语（中文）.txt']\n",
    "             #'../sentiment_dicts/NTUSD_negative_simplified.txt']\n",
    "neg_words = []\n",
    "for n in neg_files:\n",
    "    with open(n) as negFile:\n",
    "        words = negFile.readlines()\n",
    "        words = list(map(lambda x:x.strip(),words))\n",
    "        #print len(posWords)\n",
    "        neg_words.extend(words)\n",
    "print len(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pn_composition(wordlist):\n",
    "    composition = {'pos':[], 'neg':[], 'unknown':[]}\n",
    "    for r in wordlist:\n",
    "        if r.encode('utf-8') in pos_words:\n",
    "            composition['pos'].append(r)\n",
    "        elif r.encode('utf-8') in neg_words:\n",
    "            composition['neg'].append(r)\n",
    "        else:\n",
    "            composition['unknown'].append(r)\n",
    "    return composition\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_neg_ratio(wordlist):\n",
    "    composition = {'pos':[], 'neg':[], 'unknown':[]}\n",
    "    for r in wordlist:\n",
    "        if r.encode('utf-8') in pos_words:\n",
    "            composition['pos'].append(r)\n",
    "        elif r.encode('utf-8') in neg_words:\n",
    "            composition['neg'].append(r)\n",
    "        else:\n",
    "            composition['unknown'].append(r)\n",
    "    try:\n",
    "        return len(composition['pos']) / float(len(composition['neg']))\n",
    "    except ZeroDivisionError:\n",
    "        return len(composition['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 15:41:38,638 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "复新 0.720214188099\n",
      "北新 0.711165308952\n",
      "全军覆没 0.70276850462\n",
      "大举进攻 0.701167345047\n",
      "九一八事 0.694559633732\n",
      "正面 0.691871166229\n",
      "南大 0.681910037994\n",
      "反动势力 0.6732006073\n",
      "壮烈 0.671368598938\n",
      "钦差大臣 0.667848944664\n",
      "连续 0.644109845161\n",
      "公开 0.59958344698\n",
      "恐怖 0.588566124439\n",
      "黄 0.584655404091\n",
      "轰轰烈烈 0.583952784538\n",
      "恐慌 0.583698689938\n",
      "举行会谈 0.582687854767\n",
      "危急 0.577298045158\n",
      "不久 0.570244193077\n",
      "前仆后继 0.567859768867\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "word = u'蒋介石'\n",
    "#word = u'蒋中正'\n",
    "adj_word2vec = find_topn_adj(n=20, model= model_word2vec, word= word)['word']\n",
    "score_word2vec = find_topn_adj(n=20, model= model_word2vec, word=word)['score']\n",
    "for a,s in zip(adj_word2vec, score_word2vec):\n",
    "    print a, s\n",
    "print pos_neg_ratio(adj_word2vec)\n",
    "# for p in pn_composition(adj_word2vec)['neg']:\n",
    "#     print p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老 0.590997099876\n",
      "乐观 0.564539074898\n",
      "亲密 0.544198811054\n",
      "整肃 0.531194269657\n",
      "最高 0.529323041439\n",
      "独秀 0.517952919006\n",
      "大中 0.51092684269\n",
      "言者无罪 0.501269698143\n",
      "面红 0.490559220314\n",
      "公开 0.465627282858\n",
      "基 0.461378455162\n",
      "年轻 0.441419959068\n",
      "旧 0.427567183971\n",
      "具体 0.426327526569\n",
      "正确 0.42267537117\n",
      "束手无策 0.415853440762\n",
      "不满情绪 0.408960044384\n",
      "牛鬼蛇神 0.400862455368\n",
      "宽松 0.3914026618\n",
      "大势已去 0.387609213591\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "word = u'毛泽东'\n",
    "adj_word2vec = find_topn_adj(n=20, model= model_word2vec, word= word)['word']\n",
    "score_word2vec = find_topn_adj(n=20, model= model_word2vec, word=word)['score']\n",
    "for a,s in zip(adj_word2vec, score_word2vec):\n",
    "    print a, s\n",
    "print pos_neg_ratio(adj_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大元 0.544372677803\n",
      "薄弱 0.540436983109\n",
      "成功 0.518168926239\n",
      "容 0.5066857934\n",
      "落实 0.487409770489\n",
      "北新 0.487395226955\n",
      "著名 0.441380292177\n",
      "激愤 0.425225615501\n",
      "不久 0.423867881298\n",
      "独秀 0.413086593151\n",
      "封建 0.400092869997\n",
      "长 0.396668374538\n",
      "大党 0.373572409153\n",
      "确实 0.368044346571\n",
      "束手无策 0.367626935244\n",
      "刀枪不入 0.362145274878\n",
      "不慎 0.35793274641\n",
      "大势已去 0.354604721069\n",
      "重新 0.352033913136\n",
      "最高 0.338831812143\n",
      "1.25\n"
     ]
    }
   ],
   "source": [
    "word = u'孙中山'\n",
    "adj_word2vec = find_topn_adj(n=20, model= model_word2vec, word= word)['word']\n",
    "score_word2vec = find_topn_adj(n=20, model= model_word2vec, word=word)['score']\n",
    "for a,s in zip(adj_word2vec, score_word2vec):\n",
    "    print a, s\n",
    "print pos_neg_ratio(adj_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "独秀 0.702444255352\n",
      "广泛 0.673004627228\n",
      "充满 0.621963143349\n",
      "不良 0.604296207428\n",
      "着重 0.594822227955\n",
      "封建 0.592111170292\n",
      "不适 0.590828537941\n",
      "优越 0.586958408356\n",
      "著名 0.575307369232\n",
      "各阶 0.573041558266\n",
      "偷工减料 0.565737724304\n",
      "古 0.563042640686\n",
      "轻重 0.557736873627\n",
      "亲密 0.557665109634\n",
      "乐观 0.554307103157\n",
      "牛鬼蛇神 0.544725000858\n",
      "中大 0.532047152519\n",
      "恶劣 0.527855932713\n",
      "言者无罪 0.52551484108\n",
      "混乱 0.521520376205\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "word = u'马克思'\n",
    "adj_word2vec = find_topn_adj(n=20, model= model_word2vec, word= word)['word']\n",
    "score_word2vec = find_topn_adj(n=20, model= model_word2vec, word=word)['score']\n",
    "for a,s in zip(adj_word2vec, score_word2vec):\n",
    "    print a, s\n",
    "print pos_neg_ratio(adj_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "束手无策 0.601246595383\n",
      "容 0.517170786858\n",
      "重建 0.495835900307\n",
      "薄弱 0.479796588421\n",
      "壮大 0.475466251373\n",
      "健康 0.470888257027\n",
      "精锐 0.467885673046\n",
      "坚决 0.461356163025\n",
      "强烈不满 0.455766826868\n",
      "各阶 0.425942242146\n",
      "持久 0.425493776798\n",
      "强硬 0.412223219872\n",
      "速战速决 0.403032541275\n",
      "不利 0.397700428963\n",
      "小 0.395010739565\n",
      "强大 0.36047488451\n",
      "大势已去 0.357838153839\n",
      "彻底 0.356023609638\n",
      "烈 0.34965634346\n",
      "严格 0.348642766476\n",
      "3.5\n"
     ]
    }
   ],
   "source": [
    "word = u'国民党'\n",
    "adj_word2vec = find_topn_adj(n=20, model= model_word2vec, word= word)['word']\n",
    "score_word2vec = find_topn_adj(n=20, model= model_word2vec, word=word)['score']\n",
    "for a,s in zip(adj_word2vec, score_word2vec):\n",
    "    print a, s\n",
    "print pos_neg_ratio(adj_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "薄弱 0.56472837925\n",
      "独秀 0.562929868698\n",
      "坚决 0.519235610962\n",
      "团结 0.503582775593\n",
      "团结一致 0.465173721313\n",
      "基 0.46359410882\n",
      "最高 0.456141322851\n",
      "大势已去 0.443681567907\n",
      "容 0.440776407719\n",
      "大党 0.436806231737\n",
      "精锐 0.430891007185\n",
      "落实 0.418730914593\n",
      "公开 0.418326675892\n",
      "不良 0.413984686136\n",
      "一大 0.412838459015\n",
      "积极参与 0.400486648083\n",
      "束手无策 0.400304853916\n",
      "不稳 0.396662384272\n",
      "成功 0.396431148052\n",
      "各阶 0.386137485504\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "word = u'共产党'\n",
    "adj_word2vec = find_topn_adj(n=20, model= model_word2vec, word= word)['word']\n",
    "score_word2vec = find_topn_adj(n=20, model= model_word2vec, word=word)['score']\n",
    "for a,s in zip(adj_word2vec, score_word2vec):\n",
    "    print a, s\n",
    "print pos_neg_ratio(adj_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pn_ratio = []\n",
    "# for num in range(0, len(adj_word2vec), 5):\n",
    "#     adj_word2vec = find_topn_adj(n=num, model= model_word2vec, word=u'蒋介石')['word']\n",
    "#     #print pos_neg_ratio(adj_word2vec)\n",
    "#     pn_ratio.append(pos_neg_ratio(adj_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "composition = pn_composition(adj_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words_of_interest = [u'蒋介石', u'毛泽东', u'孙中山', u'国民党', u'共产党', u'马克思']\n",
    "words_of_interest = [u'蒋中正', u'毛泽东', u'孙中山', u'国民党', u'共产党', u'马克思']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj = {'center':{},'pos':{}, 'neg':{}, 'unknown':{}}\n",
    "for woi in words_of_interest[:]:\n",
    "    adj['center'][woi] = list(model_word2vec[woi])\n",
    "    adj_word2vec = find_topn_adj(n=50, model= model_word2vec, word=woi)['word']\n",
    "    composition = pn_composition(adj_word2vec)\n",
    "    for word in composition['pos']:\n",
    "        wordvec = list(model_word2vec[word])\n",
    "        if word not in adj['pos']:\n",
    "            adj['pos'][word] = wordvec\n",
    "    for word in composition['neg']:\n",
    "        wordvec = list(model_word2vec[word])\n",
    "        if word not in adj['neg']:\n",
    "            adj['neg'][word] = wordvec\n",
    "    for word in composition['unknown']:\n",
    "        wordvec = list(model_word2vec[word])\n",
    "        if word not in adj['unknown']:\n",
    "            adj['unknown'][word] = wordvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../tsne-word-embedding/tw.txt', 'wb') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter='\\t')\n",
    "    for w in adj['center']:\n",
    "        writer.writerow([w.encode('utf-8')] + adj['center'][w])\n",
    "    for w in adj['pos']:\n",
    "        #print [w] + adj['pos'][w]\n",
    "        writer.writerow([w.encode('utf-8')] + adj['pos'][w])\n",
    "    for w in adj['neg']:\n",
    "        writer.writerow([w.encode('utf-8')] + adj['neg'][w])\n",
    "    for w in adj['unknown']:\n",
    "        writer.writerow([w.encode('utf-8')] + adj['unknown'][w])\n",
    "#     for w in model_word2vec.vocab.keys()[:2000]:\n",
    "#         writer .writerow([w.encode('utf-8')] + list(model_word2vec[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
